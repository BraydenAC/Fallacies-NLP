from transformers import pipeline
from huggingface_hub import login
import re

print("Stuff loading here")


text = "We know that the earth is flat because it looks and feels flat."
#https://huggingface.co/q3fer/distilbert-base-fallacy-classification
classifier_model_path = "q3fer/distilbert-base-fallacy-classification"
q3fer_pipe = pipeline("text-classification", model=classifier_model_path, tokenizer=classifier_model_path)

#Log in to HuggingFace, return 1 if login works, return 0 and print the error if it doesn't
def hf_login(token):
    global llama_pipe
    try:
        login(token=token)
        #https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct
        gen_model_path = "meta-llama/Llama-3.2-1B-Instruct"
        llama_pipe = pipeline("text-generation", model=gen_model_path)
        return 1
    except Exception as e:
        print(f"Error logging in: {e}")
        del llama_pipe
        return 0

#Process the input when called by server.
#Take a string, predict whether it is a fallacy, and if so what kind of fallacy it is.
#Then, take the predicted fallacy and pass it to llama to generate an explanation as to why it is a fallacy.
def process_input(input):

    #Run input through pipeline
    result = q3fer_pipe(input)

    pred = result[0]['label']
    #If the model predicts that the input is not a fallacy, 
    threshold = 0.5
    if result[0]['score'] <= threshold:
        pred = "No fallacy detected!"
        return pred + " with a score of " + str(result[0]['score'])
    try:
        message = f"The input is a {pred} fallacy. Please explain why this is a fallacy.\n\nInput: {input}\n\nExplanation:" # <- Generated by ChatGPT
        explanation = llama_pipe(message, max_length=225)
        formatted_explanation = re.split(r"Explanation.*\n\n", explanation[0]["generated_text"])
        return formatted_explanation[1].strip() + "..."
    except Exception as e:
        print(f"Error generating explanation: {e}")
        return f"An error was encountered while generating a full explanation, but a {pred} fallacy was detected."
# print(hf_login("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"))